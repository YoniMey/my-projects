<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jonathan Mey-Tal's Projects</title>
  <style>
    /* --- Original Styles --- */
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background: #f4f4f4;
      color: #333;
    }
    header {
      background: #004f90;
      color: #fff;
      padding: 2rem 1rem;
      text-align: center;
    }
    header h1 {
      margin: 0;
      font-size: 2.5rem;
    }
    main {
      padding: 2rem;
      max-width: 900px;
      margin: auto;
      background: #fff;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      border-radius: 8px;
    }
    section {
      margin-bottom: 3rem;
    }
    h2 {
      border-left: 5px solid #004f90;
      padding-left: 1rem;
      margin-top: 0;
      color: #004f90;
    }
    .year {
      font-weight: normal;
      font-size: 0.9rem;
      color: #666;
    }
    ul, ol {
      padding-left: 1.5rem;
    }
    ul li, ol li {
      margin-bottom: 0.5rem;
    }
    footer {
      text-align: center;
      padding: 1rem;
      background: #004f90;
      color: white;
    }
    img {
      display: block;
      margin: 1.5rem auto 0 auto;
      max-width: 100%;
      border-radius: 4px;
      box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);
    }

    /* --- New Styles for Modal and Slider --- */
    .open-modal-btn {
      background-color: #004f90;
      color: white;
      border: none;
      padding: 0.75rem 1.5rem;
      border-radius: 5px;
      cursor: pointer;
      font-size: 1rem;
      margin-top: 1rem;
      display: inline-block;
      transition: background-color 0.3s;
    }

    .open-modal-btn:hover {
        background-color: #003366;
    }

    .modal {
      display: none; /* Hidden by default */
      position: fixed; 
      z-index: 1000;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      overflow: auto;
      background-color: rgba(0,0,0,0.6);
      align-items: center;
      justify-content: center;
    }

    .modal.active {
        display: flex;
    }

    .modal-content {
      background-color: #fff;
      margin: auto;
      padding: 20px;
      border: 1px solid #888;
      width: 90%;
      max-width: 800px;
      border-radius: 8px;
      position: relative;
      box-shadow: 0 5px 15px rgba(0,0,0,0.3);
    }

    .modal-content video {
      width: 100%;
      height: auto;
      border-radius: 4px;
      margin-top: 1rem;
    }

    .close-modal-btn {
      color: #333;
      position: absolute;
      top: 10px;
      right: 25px;
      font-size: 35px;
      font-weight: bold;
      cursor: pointer;
      z-index: 1;
    }

    /* Slider-specific styles */
    .slider {
      position: relative;
      width: 100%;
      overflow: hidden;
    }

    .slides {
      display: flex;
      transition: transform 0.5s ease-in-out;
    }

    .slide {
      flex: 0 0 100%;
      box-sizing: border-box;
    }

    .slide img {
      width: 100%;
      display: block;
    }

    .slider-btn {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      background-color: rgba(0, 0, 0, 0.5);
      color: white;
      border: none;
      cursor: pointer;
      padding: 16px;
      font-size: 18px;
      z-index: 100;
      border-radius: 50%;
      width: 48px;
      height: 48px;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .slider-btn.prev {
      left: 10px;
    }

    .slider-btn.next {
      right: 10px;
    }
  </style>
</head>
<body>
  <header>
    <h1>Jonathan Mey-Tal's Projects</h1>
  </header>

  <main>
    <section>
      <h2>My Thesis <span class="year">(2025)</span></h2>
      <p><strong>Goal:</strong> Build n accurate and consistent LLM-based agent system capable of answering and reasoning about complex 
                                temporal questions grounded in a given document.</p>
      <p><strong>Core Idea:</strong> Convert a document into a time focused RDF knowledge graph, which can be queried using SPARQL to retrieve 
                                the desired information. I use the DSPy framework to integrate the different LLM modules and agents, and for their 
                                prompt optimization methods.</p>
      <p>The project is built from the following main components:</p>
      <ul>
        <li><strong>OWL Schema Generation:</strong> An LLM module (implemented in DSPy and optimized using a small generative dataset I created) 
                                produces an OWL schema that captures the core structure and concepts of the text. The schema defines the
                                entity classes and possible relations for the final graph.</li>
        <li><strong>Entity Extraction & Linking:</strong> Based on the schema and the document, I use GLiNER for entity extraction and
                                ReLiK for both entity linking and relation extraction.</li>
        <li><strong>RDF Graph Construction:</strong> This is an iterative loop: an LLM generates a candidate graph, which is then validated
                                using a formal RDF parser (rdflib). Another LLM generates questions about the original text, attempts to answer them
                                using SPARQL queries on the graph, and identifies inconsistencies. These are fed back into the graph generation module
                                to refine and improve the graph.</li>
        <li><strong>SPARQL Query Generation:</strong> To translate natural language questions into SPARQL queries, I use Spinach—an agent-
                                based method that learns and navigates the structure of the RDF graph to produce accurate queries.</li>
      </ul>
    </section>

    <section>
      <h2>AudioCorrect <span class="year">(2025)</span></h2>
      <p>This project explores the use of speech and audio AI models to create an app that can correct mispronounced or incorrect
        words in an audio recording. Given a main audio file and reference recordings, the system transcribes the content and allows for
        text-level editing. The output is a modified version of the original recording, with new segments inserted in the main record.
        The process consists of the following steps:</p>
      <ol>
        <li>ranscribe the input audio and extract word-level timings using WhisperX.</li>
        <li>Mark sections to edit (equal/delete/replace/insert).</li>
        <li>Generate the required segments using a TTS model. I implemented two options: (1) Tortoise TTS (local, but slow and
            lower quality), and (2) ElevenLabs API (faster and higher quality).</li>
        <li>Reassemble the modified audio: to ensure smoother transitions, I applied volume adjustments, silence padding, and fade
            effects between overlapping segments.</li>
      </ol>
      <p>Reassemble the modified audio: to ensure smoother transitions, I applied volume adjustments, silence padding, and fade
         effects between overlapping segments.</p>
        <!-- Button to open the image slider modal -->
      <button class="open-modal-btn" data-modal-target="#audiocorrect-slider-modal">View Images</button>

      <!-- The Slider Modal for AudioCorrect -->
      <div class="modal" id="audiocorrect-slider-modal">
        <div class="modal-content">
          <span class="close-modal-btn">&times;</span>
          <div class="slider">
            <div class="slides">
              <!-- Slide 1 -->
              <div class="slide">
                <img src="images/AudioCorrecrAPP.png" alt="AudioCorrect App main screen" onerror="this.onerror=null;this.src='https://placehold.co/800x600/e2e8f0/4a5568?text=Image+Not+Found';">
              </div>
              <!-- Slide 2 -->
              <div class="slide">
                <img src="images/AudioCorrecrAPP1.png" alt="AudioCorrect workflow diagram" onerror="this.onerror=null;this.src='https://placehold.co/800x600/e2e8f0/4a5568?text=Image+Not+Found';">
              </div>
              <!-- Slide 3 (Add a placeholder image) -->
              <div class="slide">
                <img src="images/AudioCorrecrAPP2.png" alt="Editing view in AudioCorrect" onerror="this.onerror=null;this.src='https://placehold.co/800x600/e2e8f0/4a5568?text=Image+Not+Found';">
              </div>
            </div>
            <!-- Next and previous buttons -->
            <button class="slider-btn prev">&#10094;</button>
            <button class="slider-btn next">&#10095;</button>
          </div>
        </div>
      </div>
    </section>

    <section>
      <h2>ReDress <span class="year">(2023)</span></h2>
      <p><em>2nd place - Samsung NEXT Hackathon for Mobile Generative AI</em></p>
      <p>Over one long night,we developed a demo for a mobile app for buying and selling second-hand clothing. Our approach
         leveraged CLIP, a model that maps both images and text into a shared embedding space, enabling powerful search.</p>
      <ul>
        <li><strong>Buyers side:</strong> Search is performed using natural language descriptions. The app retrieves relevant items from a vector
                                  database by comparing the description’s embedding with image embeddings eliminating the need for traditional filters or
                                  category navigation.</li>
        <li><strong>Sellers side:</strong> There’s no need to manually assign categories or write descriptions. Sellers can simply upload an image and
                                  set a price the search mechanism takes care of the rest.</li>
      </ul>
    <!-- Button to open the video modal -->
      <button class="open-modal-btn" data-modal-target="#redress-video-modal">View Demo</button>

      <!-- The Video Modal for ReDress -->
      <div class="modal" id="redress-video-modal">
        <div class="modal-content">
          <span class="close-modal-btn">&times;</span>
          <h3>ReDress App Demo</h3>
          <video controls>
            <!-- Make sure to place a video file named 'redress_demo.mp4' in your project folder -->
            <source src="Videos/ReDress_demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </section>

    <section>
      <h2>CANNABOT - AI-Powered Medical Cannabis Strain Recommender <span class="year">(2023)</span></h2>
      <p>
        Developed as a final Bachelor's Degree project in collaboration with psychiatric therapists Dr. llan Volkov and Dr. Maya Kuperberg,
        this project addresses the challenge of selecting the right medical cannabis strain from an ever-growing variety. We created an intuitive
        chatbot, CANNABOT, that leverages LLM to provide personalized strain recommendations based on patients' desired effects.
      </p>
      <p>
        The system is built on a custom database created by mining freely available user comments from online cannabis forums, providing a rich, real-world
        source of information.
      </p>

      <h4>How It Works:</h4>
      <ol>
        <li><strong>Data Mining:</strong> A multi-threaded code built with Python, and BeautifulSoup extracts thousands of user reviews from online forums on a variety of strains.</li>
        <li><strong>AI-Powered Data Tagging:</strong> Using prompt engineering and the OpenAI API, each review is tagged with a list of predefined effects (created by our domain experts) to structure the unstructured text.</li>
        <li><strong>Database Construction:</strong> The system parses the API's response to build a database, mapping strains to specific effects with a corresponding probability score.</li>
        <li><strong>Processing a user request:</strong> A retrieval system processes user requests. It retrieves the most relevant strain from our database according to the extracted effects from the user request and uses the OpenAI API to generate an accurate, natural-language recommendation.</li>
        <li><strong>WhatsApp Integration:</strong> Users can interact with CANNABOT seamlessly through WhatsApp, thanks to the Twilio API, making it accessible and easy to use.</li>
      </ul>

      <!-- Button to open the image slider modal -->
      <button class="open-modal-btn" data-modal-target="#cannabot-slider-modal">View Images</button>

      <!-- The Slider Modal for CANNABOT -->
      <div class="modal" id="cannabot-slider-modal">
        <div class="modal-content">
          <span class="close-modal-btn">&times;</span>
          <div class="slider">
            <div class="slides">
              <!-- Add your images for the CANNABOT project here -->
              <div class="slide">
                <img src="images/CANABOT_demo.png" alt="CANNABOT chat interface" onerror="this.onerror=null;this.src='https://placehold.co/800x600/e2e8f0/4a5568?text=Image+1+Not+Found';">
              </div>
              <div class="slide">
                <img src="images/CANABOT_team.png" alt="CANNABOT system architecture" onerror="this.onerror=null;this.src='https://placehold.co/800x600/e2e8f0/4a5568?text=Image+2+Not+Found';">
              </div>
            </div>
            <!-- Next and previous buttons -->
            <button class="slider-btn prev">&#10094;</button>
            <button class="slider-btn next">&#10095;</button>
          </div>
        </div>
      </div>
    </section>
  </main>

  <footer>
    <p>© 2025 Jonathan Mey-Tal</p>
  </footer>

  <!-- JAVASCRIPT FOR MODAL AND SLIDER -->
  <!-- Placed at the end of the body to ensure all HTML elements are loaded first -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // --- Modal opening/closing logic ---
      const openModalButtons = document.querySelectorAll('.open-modal-btn');
      const closeModalButtons = document.querySelectorAll('.close-modal-btn');

      // Add click listener to all "Open Modal" buttons
      openModalButtons.forEach(button => {
        button.addEventListener('click', () => {
          // Find the modal specified in the button's 'data-modal-target' attribute
          const modal = document.querySelector(button.dataset.modalTarget);
          openModal(modal);
        });
      });

      // Add click listener to all "Close" (X) buttons
      closeModalButtons.forEach(button => {
        button.addEventListener('click', () => {
          // Find the closest parent with the 'modal' class and close it
          const modal = button.closest('.modal');
          closeModal(modal);
        });
      });

      // Add click listener to the window to close modal if clicking outside the content
      window.addEventListener('click', event => {
          document.querySelectorAll('.modal.active').forEach(modal => {
              if (event.target == modal) {
                  closeModal(modal);
              }
          });
      });

      function openModal(modal) {
        if (modal == null) return;
        modal.classList.add('active'); // The 'active' class uses CSS to 'display: flex'
        
        // Reset slider to the first image if the modal contains a slider
        const slider = modal.querySelector('.slider');
        if (slider) {
            const slides = slider.querySelector('.slides');
            slides.style.transform = 'translateX(0%)';
            slider.dataset.currentIndex = 0;
        }
      }

      function closeModal(modal) {
        if (modal == null) return;
        modal.classList.remove('active');
        
        // Pause video when modal is closed to stop it from playing in the background
        const video = modal.querySelector('video');
        if (video) {
          video.pause();
        }
      }

      // --- Slider logic for all sliders on the page ---
      const sliders = document.querySelectorAll('.slider');

      sliders.forEach(slider => {
        const slides = slider.querySelector('.slides');
        const prevButton = slider.querySelector('.slider-btn.prev');
        const nextButton = slider.querySelector('.slider-btn.next');
        const slideCount = slides.children.length;
        
        // Store the current index on the slider element itself
        slider.dataset.currentIndex = 0;

        nextButton.addEventListener('click', () => {
          let currentIndex = parseInt(slider.dataset.currentIndex);
          // Move to the next slide, looping back to the start if at the end
          currentIndex = (currentIndex + 1) % slideCount;
          slider.dataset.currentIndex = currentIndex;
          // Move the .slides container horizontally
          slides.style.transform = `translateX(-${currentIndex * 100}%)`;
        });

        prevButton.addEventListener('click', () => {
          let currentIndex = parseInt(slider.dataset.currentIndex);
          // Move to the previous slide, looping to the end if at the start
          currentIndex = (currentIndex - 1 + slideCount) % slideCount;
          slider.dataset.currentIndex = currentIndex;
          slides.style.transform = `translateX(-${currentIndex * 100}%)`;
        });
      });
    });
  </script>

</body>
</html>