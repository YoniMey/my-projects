<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jonathan Mey-Tal's Projects</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background: #f4f4f4;
      color: #333;
    }
    header {
      background: #004f90;
      color: #fff;
      padding: 2rem 1rem;
      text-align: center;
    }
    header h1 {
      margin: 0;
      font-size: 2.5rem;
    }
    main {
      padding: 2rem;
      max-width: 900px;
      margin: auto;
      background: #fff;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      border-radius: 8px;
    }
    section {
      margin-bottom: 3rem;
    }
    h2 {
      border-left: 5px solid #004f90;
      padding-left: 1rem;
      margin-top: 0;
      color: #004f90;
    }
    .year {
      font-weight: normal;
      font-size: 0.9rem;
      color: #666;
    }
    ul, ol {
      padding-left: 1.5rem;
    }
    ul li, ol li {
      margin-bottom: 0.5rem;
    }
    footer {
      text-align: center;
      padding: 1rem;
      background: #004f90;
      color: white;
    }
    img {
      display: block;
      margin: 1.5rem auto 0 auto;
      max-width: 100%;
      border-radius: 4px;
      box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);
    }
  </style>
</head>
<body>
  <header>
    <h1>Jonathan Mey-Tal's Projects</h1>
  </header>

  <main>
    <section>
      <h2>My Thesis <span class="year">(2025)</span></h2>
      <p><strong>Goal:</strong> Build n accurate and consistent LLM-based agent system capable of answering and reasoning about complex 
                                temporal questions grounded in a given document.</p>
      <p><strong>Core Idea:</strong> Convert a document into a time focused RDF knowledge graph, which can be queried using SPARQL to retrieve 
                                the desired information. I use the DSPy framework to integrate the different LLM modules and agents, and for their 
                                prompt optimization methods.</p>
      <p>The project is built from the following main components:</p>
      <ul>
        <li><strong>OWL Schema Generation:</strong> An LLM module (implemented in DSPy and optimized using a small generative dataset I created) 
                                produces an OWL schema that captures the core structure and concepts of the text. The schema defines the
                                entity classes and possible relations for the final graph.</li>
        <li><strong>Entity Extraction & Linking:</strong> Based on the schema and the document, I use GLiNER for entity extraction and
                                ReLiK for both entity linking and relation extraction.</li>
        <li><strong>RDF Graph Construction:</strong> This is an iterative loop: an LLM generates a candidate graph, which is then validated
                                using a formal RDF parser (rdflib). Another LLM generates questions about the original text, attempts to answer them
                                using SPARQL queries on the graph, and identifies inconsistencies. These are fed back into the graph generation module
                                to refine and improve the graph.</li>
        <li><strong>SPARQL Query Generation:</strong> To translate natural language questions into SPARQL queries, I use Spinach—an agent-
                                based method that learns and navigates the structure of the RDF graph to produce accurate queries.</li>
      </ul>
    </section>

    <section>
      <h2>AudioCorrect <span class="year">(2025)</span></h2>
      <p>This project explores the use of speech and audio AI models to create an app that can correct mispronounced or incorrect
        words in an audio recording. Given a main audio file and reference recordings, the system transcribes the content and allows for
        text-level editing. The output is a modified version of the original recording, with new segments inserted in the main record.
        The process consists of the following steps:</p>
      <ol>
        <li>ranscribe the input audio and extract word-level timings using WhisperX.</li>
        <li>Mark sections to edit (equal/delete/replace/insert).</li>
        <li>Generate the required segments using a TTS model. I implemented two options: (1) Tortoise TTS (local, but slow and
            lower quality), and (2) ElevenLabs API (faster and higher quality).</li>
        <li>Reassemble the modified audio: to ensure smoother transitions, I applied volume adjustments, silence padding, and fade
            effects between overlapping segments.</li>
      </ol>
      <p>Reassemble the modified audio: to ensure smoother transitions, I applied volume adjustments, silence padding, and fade
         effects between overlapping segments.</p>
        <button class="open-modal-btn" data-modal-target="#audiocorrect-slider-modal">View Images</button>

      <div class="modal" id="audiocorrect-slider-modal">
        <div class="modal-content">
          <span class="close-modal-btn">&times;</span>
          <div class="slider">
            <div class="slides">
              <div class="slide">
                <img src="AudioCorrectAPP.png" alt="AudioCorrect App main screen">
              </div>
              <div class="slide">
                <img src="AudioCorrectAPP2.png" alt="AudioCorrect workflow diagram">
              </div>
              <div class="slide">
                <img src="AudioCorrectAPP3.png" alt="Editing view in AudioCorrect">
              </div>
            </div>
            <button class="slider-btn prev">&#10094;</button>
            <button class="slider-btn next">&#10095;</button>
          </div>
        </div>
      </div>
    </section>

    <section>
      <h2>ReDress <span class="year">(2023)</span></h2>
      <p><em>2nd place - Samsung NEXT Hackathon for Mobile Generative AI</em></p>
      <p>During the Samsung NEXT Hackathon, we built a prototype app for buying and selling second-hand clothing. Our approach
         leveraged CLIP, a model that maps both images and text into a shared embedding space, enabling powerful search.</p>
      <ul>
        <li><strong>Buyers side:</strong> Search is performed using natural language descriptions. The app retrieves relevant items from a vector
                                  database by comparing the description’s embedding with image embeddings eliminating the need for traditional filters or
                                  category navigation.</li>
        <li><strong>Sellers side:</strong> There’s no need to manually assign categories or write descriptions. Sellers can simply upload an image and
                                  set a price the search mechanism takes care of the rest.</li>
      </ul>
    </section>
  </main>

  <footer>
    <p>© 2025 Jonathan Mey-Tal</p>
  </footer>

  <!-- JAVASCRIPT FOR MODAL AND SLIDER -->
  <!-- Placed at the end of the body to ensure all HTML elements are loaded first -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // --- Modal opening/closing logic ---
      const openModalButtons = document.querySelectorAll('.open-modal-btn');
      const closeModalButtons = document.querySelectorAll('.close-modal-btn');

      // Add click listener to all "Open Modal" buttons
      openModalButtons.forEach(button => {
        button.addEventListener('click', () => {
          // Find the modal specified in the button's 'data-modal-target' attribute
          const modal = document.querySelector(button.dataset.modalTarget);
          openModal(modal);
        });
      });

      // Add click listener to all "Close" (X) buttons
      closeModalButtons.forEach(button => {
        button.addEventListener('click', () => {
          // Find the closest parent with the 'modal' class and close it
          const modal = button.closest('.modal');
          closeModal(modal);
        });
      });

      // Add click listener to the window to close modal if clicking outside the content
      window.addEventListener('click', event => {
          document.querySelectorAll('.modal.active').forEach(modal => {
              if (event.target == modal) {
                  closeModal(modal);
              }
          });
      });

      function openModal(modal) {
        if (modal == null) return;
        modal.classList.add('active'); // The 'active' class uses CSS to 'display: flex'
        
        // Reset slider to the first image every time the modal is opened
        const slider = modal.querySelector('.slider');
        if (slider) {
            const slides = slider.querySelector('.slides');
            slides.style.transform = 'translateX(0%)';
            slider.dataset.currentIndex = 0;
        }
      }

      function closeModal(modal) {
        if (modal == null) return;
        modal.classList.remove('active');
      }

      // --- Slider logic for all sliders on the page ---
      const sliders = document.querySelectorAll('.slider');

      sliders.forEach(slider => {
        const slides = slider.querySelector('.slides');
        const prevButton = slider.querySelector('.slider-btn.prev');
        const nextButton = slider.querySelector('.slider-btn.next');
        const slideCount = slides.children.length;
        
        // Store the current index on the slider element itself
        slider.dataset.currentIndex = 0;

        nextButton.addEventListener('click', () => {
          let currentIndex = parseInt(slider.dataset.currentIndex);
          // Move to the next slide, looping back to the start if at the end
          currentIndex = (currentIndex + 1) % slideCount;
          slider.dataset.currentIndex = currentIndex;
          // Move the .slides container horizontally
          slides.style.transform = `translateX(-${currentIndex * 100}%)`;
        });

        prevButton.addEventListener('click', () => {
          let currentIndex = parseInt(slider.dataset.currentIndex);
          // Move to the previous slide, looping to the end if at the start
          currentIndex = (currentIndex - 1 + slideCount) % slideCount;
          slider.dataset.currentIndex = currentIndex;
          slides.style.transform = `translateX(-${currentIndex * 100}%)`;
        });
      });
    });
  </script>


</body>
</html>
